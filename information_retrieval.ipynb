{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval System\n",
    "\n",
    "Task: Develop an information retrieval system based on ranked retrieval. The intended system should be based on tf-idf scores and cosine similarities to retrieve ranked indices of documents most relevant to the need. \n",
    "\n",
    "Upon querying, the query should be compared to the words of every document based on the mentioned scheme and returns ranked indices most relevant to the query. The 5 most relevant documents should be returned. If there are fewer than 5 relevant documents then only relevant documents should be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtain stopwords list and define stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "nltk_stopwords = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(nltk_stopwords[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load queries and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['25 batman alone man', 'lack of intelligence', 'game of soccer', 'undertaker wwe record', 'movie for kids', 'harry kane height']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiker, demon, creepy, scary, tunnel, stalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Batman, batman beyond, who are you, narrows it down, animated, show, officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Up, carl, russell, honor, award, scout badge, old man, kids, movie, record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom, jerry, sword, stab, dont care, cartoon, show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wholesome, comic, dialogue bubble, dog, sleeping with owner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              words\n",
       "docID                                                                              \n",
       "0                                        Hiker, demon, creepy, scary, tunnel, stalk\n",
       "1      Batman, batman beyond, who are you, narrows it down, animated, show, officer\n",
       "2        Up, carl, russell, honor, award, scout badge, old man, kids, movie, record\n",
       "3                                 Tom, jerry, sword, stab, dont care, cartoon, show\n",
       "4                       Wholesome, comic, dialogue bubble, dog, sleeping with owner"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Queries.txt','r') as f:\n",
    "    queries = f.read()\n",
    "queries_list = re.split('\\n',queries)\n",
    "queries_list = [query for query in queries_list if query]\n",
    "print(queries_list)\n",
    "\n",
    "documents=pd.read_csv(\"WordsDataset.csv\", index_col=0)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Cosine similarity function to be used to determine the similarity between the query and each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    numerator = np.dot(vector1,vector2)\n",
    "    denominator = np.linalg.norm(vector1)*np.linalg.norm(vector2)\n",
    "    if denominator == 0:\n",
    "        similarity = 0\n",
    "    else:\n",
    "        similarity = numerator/denominator\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document retrieval function to return the 10 most relevant documents from 'WordsDataset.csv' file\n",
    "- Document vectors are obtained for each document in the data set. \n",
    "- Each value in the document vector is the Term Frequency-Inverse Document Frequency (TF-IDF) weight for a given word in the shared vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query,documents):\n",
    "\n",
    "    print(\"Query: \" + query)\n",
    "\n",
    "    # populate dictionary with each indexed document\n",
    "    # Tokenise, apply stopwords and stem each document\n",
    "    documents_dict = {}\n",
    "    for i in range(len(documents)):\n",
    "        tokens = word_tokenize(documents.loc[i]['words'])\n",
    "        words = [word for word in tokens if word.isalnum()]\n",
    "        words = [word for word in words if not word in stopwords.words()]\n",
    "        words = [ps.stem(word) for word in words]\n",
    "        documents_dict[i] = words\n",
    "\n",
    "    # tokenised, stopped and stemmed query\n",
    "    query_tokens = word_tokenize(query)\n",
    "    query_words = [word for word in query_tokens if word.isalnum()]\n",
    "    query_words = [word for word in query_words if not word in stopwords.words()]\n",
    "    query_words = [ps.stem(word) for word in query_words]\n",
    "\n",
    "    # Obtain inverse document frequency for each term in shared vector space\n",
    "    # shared vector space is the vector indexed by relevant words in the query\n",
    "    idf_term = []\n",
    "    for word in query_words:\n",
    "        doc_count = 0\n",
    "        for i in range(len(documents)):\n",
    "            if word in documents_dict[i]:\n",
    "                doc_count += 1\n",
    "        \n",
    "        idf_term.append(np.log10(len(documents)/(1 + doc_count)))\n",
    "\n",
    "    # creating empty term frequency and document vectors for each document\n",
    "    termfrequency_dict = {}\n",
    "    docvector_dict = {}\n",
    "    for i in range(len(documents)):\n",
    "        termfrequency_dict[i] = []\n",
    "        docvector_dict[i] = []\n",
    "        for k in range(len(query_words)):\n",
    "            termfrequency_dict[i].append(0)\n",
    "            docvector_dict[i].append(0)\n",
    "\n",
    "    # populating document vectors for each document\n",
    "    # value is given by tf-idf weight for each term in shared vector space\n",
    "    for i in range(len(documents)):\n",
    "        k=0\n",
    "        for word in query_words:\n",
    "            for term in documents_dict[i]:\n",
    "                if term == word:\n",
    "                    termfrequency_dict[i][k] += 1\n",
    "            docvector_dict[i][k] = termfrequency_dict[i][k]*idf_term[k]\n",
    "            k += 1\n",
    "\n",
    "    # populating query document vector\n",
    "    query_docvector = [1 for word in query_words]\n",
    "\n",
    "    # Find the cosine similarities between each document and the query\n",
    "    doc_similarities = {}\n",
    "    for i in range(len(documents)):\n",
    "        doc_similarities[i] = cosine_similarity(docvector_dict[i], query_docvector)\n",
    "\n",
    "    # Sort the documents in order of the most relevant document\n",
    "    sorted_doc_sim = sorted(doc_similarities.items(), key=lambda x:x[1], reverse=True)\n",
    "    sorted_doc_sim = sorted_doc_sim[0:5]\n",
    "\n",
    "    relevant_queries_idx = []\n",
    "    for i in range(len(sorted_doc_sim)):\n",
    "        if sorted_doc_sim[i][1] > 0: # Do not want to return documents that have 0 similarity\n",
    "            relevant_queries_idx.append(sorted_doc_sim[i][0])\n",
    "\n",
    "    print(\"Search results:\")\n",
    "    print(documents.loc[relevant_queries_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 25 batman alone man\n",
      "Search results:\n",
      "                                                                              words\n",
      "docID                                                                              \n",
      "1      Batman, batman beyond, who are you, narrows it down, animated, show, officer\n",
      "35                                                    Uno, draw 25, option, classic\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "retrieve_documents(queries_list[0],documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: lack of intelligence\n",
      "Search results:\n",
      "                                                 words\n",
      "docID                                                 \n",
      "28     Stupid intelligence, brain, what is this answer\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents(queries_list[1],documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: game of soccer\n",
      "Search results:\n",
      "                                                                  words\n",
      "docID                                                                  \n",
      "8      Geralt, yennefer, pointing, blame, slapstick, video game, player\n",
      "53                      video game, football, harry kane, umpire, sport\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents(queries_list[2],documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: undertaker wwe record\n",
      "Search results:\n",
      "                                                                            words\n",
      "docID                                                                            \n",
      "34                              Undertaker, randy Orton, surprised, reaction, wwe\n",
      "2      Up, carl, russell, honor, award, scout badge, old man, kids, movie, record\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents(queries_list[3],documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: movie for kids\n",
      "Search results:\n",
      "                                                                            words\n",
      "docID                                                                            \n",
      "2      Up, carl, russell, honor, award, scout badge, old man, kids, movie, record\n",
      "7                 Lord of the rings, lotr, gandalf, pipip, sending, movie, height\n",
      "11                          Groot, gunpoint, force, surreal, movie, despicable me\n",
      "15                  Jedi, master, lightsaber, block, unexpected, movie, star wars\n",
      "16                         Joker, you think this is funny, laugh, reaction, movie\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents(queries_list[4],documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: harry kane height\n",
      "Search results:\n",
      "                                                                 words\n",
      "docID                                                                 \n",
      "53                     video game, football, harry kane, umpire, sport\n",
      "7      Lord of the rings, lotr, gandalf, pipip, sending, movie, height\n",
      "50       Harry Osborn, peter parker, spider man, movie, hidden, voeyer\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents(queries_list[5],documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5417c8c9055c1fa8caa20bdcdcb9a94277d5725cf87b106d0a354dab992ca3fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
